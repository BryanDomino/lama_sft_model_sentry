{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2fd0900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import transformers\n",
    "\n",
    "class MyModel(mlflow.pyfunc.PythonModel):\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        import os\n",
    "        import torch\n",
    "        from transformers import (\n",
    "            AutoModelForCausalLM,\n",
    "            AutoTokenizer,\n",
    "            BitsAndBytesConfig\n",
    "        )\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.project_id = os.listdir('/artifacts/mlflow')[0]\n",
    "        prefix = f\"{self.project_id}/version0\"\n",
    "        compute_dtype = getattr(torch, \"float16\")\n",
    "        quant_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                          bnb_4bit_quant_type=\"nf4\",\n",
    "                                          bnb_4bit_compute_dtype=compute_dtype,\n",
    "                                          bnb_4bit_use_double_quant=False)\n",
    "\n",
    "        ft_model_name = \"final_merged_checkpoint\"\n",
    "        model_cache = \"llama2-model-cache\"\n",
    "        model_tokenizer_path = f\"/artifacts/mlflow/{prefix}/{ft_model_name}\"\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_tokenizer_path,\n",
    "                                                          cache_dir=f\"/artifacts/mlflow/{prefix}/{model_cache}/\",\n",
    "                                                          quantization_config=quant_config,\n",
    "                                                          device_map=\"auto\"\n",
    "                                                          #device=\"0\"\n",
    "                                                          )\n",
    "        self.model.config.use_cache = False\n",
    "        self.model.config.pretraining_tp = 1\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_tokenizer_path, \n",
    "                                                       cache_dir=f\"/artifacts/mlflow/{prefix}/{model_cache}/\",\n",
    "                                                       trust_remote_code=True)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    def predict(self, context, model_input, params=None):\n",
    "        \"\"\"\n",
    "        This method generates prediction for the given input.\n",
    "        \"\"\"\n",
    "        prompt = model_input[\"prompt\"]\n",
    "\n",
    "        if prompt is None:\n",
    "            return 'Please provide a prompt.'\n",
    "        \n",
    "        prompt_template = f\"<s>[INST] {{dialogue}} [/INST]\"\n",
    "            \n",
    "\n",
    "        user_input = f\"<s>[INST] {prompt} [/INST]\"\n",
    "        \n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(self.tokenizer.encode(user_input))\n",
    "        input_length = len(tokens)\n",
    "        \n",
    "        new_tokens = 750\n",
    "        \n",
    "        text = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "        device = \"cuda:0\"\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        generation_config = transformers.GenerationConfig(\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    max_new_tokens = 200\n",
    "                )\n",
    "\n",
    "        outputs = self.model.generate(**inputs, generation_config=generation_config)\n",
    "        llm_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        result = llm_output.replace(f\"[INST] {prompt} [/INST]\", '')\n",
    "        return {'text_from_llm': result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e9c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, ColSpec, ParamSchema, ParamSpec\n",
    "\n",
    "# Define input and output schema\n",
    "input_schema = Schema(\n",
    "    [\n",
    "        ColSpec(DataType.string, \"prompt\"),\n",
    "    ]\n",
    ")\n",
    "output_schema = Schema([ColSpec(DataType.string, \"text_from_llm\")])\n",
    "\n",
    "parameters = ParamSchema(\n",
    "    [       \n",
    "    ]\n",
    ")\n",
    "\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=parameters)\n",
    "\n",
    "\n",
    "# Define input example\n",
    "input_example = pd.DataFrame({\"prompt\": [\"What is machine learning?\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "874a6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "model_name=\"llama2-guanaco-sft-v2\"\n",
    "registered_model = None\n",
    "try:\n",
    "    registered_model = client.create_registered_model(model_name)\n",
    "except:\n",
    "     registered_model = client.get_registered_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b030912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts:   0%|          | 0/30 [00:00<?, ?it/s]2024/03/02 11:21:38 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n",
      "Downloading artifacts: 100%|██████████| 30/30 [00:00<00:00, 2154.98it/s]\n",
      "2024/03/02 11:21:42 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: llama2-guanaco-sft-v2, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs:/f4c44f3cac764a2c921a8aa7baa9eed4/llama2-guanaco-sft-v2\n",
      "Name: llama2-guanaco-sft-v2\n",
      "Version: 1\n",
      "Description: \n",
      "Status: READY\n",
      "Stage: None\n"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "import trl\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "\n",
    "from mlflow.store.artifact.runs_artifact_repo import RunsArtifactRepository\n",
    "\n",
    "# Get the current base version of torch that is installed, without specific version modifiers\n",
    "torch_version = torch.__version__.split(\"+\")[0]\n",
    "\n",
    "# Start an MLflow run context and log the llama2-7B model wrapper along with the param-included signature to\n",
    "# allow for overriding parameters at inference time\n",
    "with mlflow.start_run() as run:\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        model_name,\n",
    "        python_model=MyModel(),\n",
    "        # NOTE: the artifacts dictionary mapping is critical! This dict is used by the load_context() method in our MyModel() class.\n",
    "        artifacts={\"snapshot\": '/mnt/code'},\n",
    "        pip_requirements=[\n",
    "            f\"torch=={torch_version}\",\n",
    "            f\"transformers=={transformers.__version__}\",                        \n",
    "            f\"peft=={peft.__version__}\",\n",
    "            f\"trl=={trl.__version__}\",            \n",
    "            \"einops\",\n",
    "            \"sentencepiece\",\n",
    "        ],\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )\n",
    "    runs_uri = model_info.model_uri\n",
    "    print(runs_uri)\n",
    "    # Create a new model version of the RandomForestRegression model from this run\n",
    "    \n",
    "    model_src = RunsArtifactRepository.get_underlying_uri(runs_uri)\n",
    "    mv = client.create_model_version(model_name, model_src, run.info.run_id)\n",
    "    print(\"Name: {}\".format(mv.name))\n",
    "    print(\"Version: {}\".format(mv.version))\n",
    "    print(\"Description: {}\".format(mv.description))\n",
    "    print(\"Status: {}\".format(mv.status))\n",
    "    print(\"Stage: {}\".format(mv.current_stage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc1648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
