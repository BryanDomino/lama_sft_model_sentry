{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce318cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import mlflow\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import gc\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "#print(os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"])\n",
    "project_id = os.environ['DOMINO_PROJECT_ID']\n",
    "new_model = \"llama-2-7b-chat-guanaco\"\n",
    "\n",
    "#Increment the version number each time you retrain it\n",
    "prefix = f\"{project_id}/version0\"\n",
    "model_folder_prefix = f\"{project_id}/model/version0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac5794c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: invalid option -- 'm'\n",
      "Try 'rm --help' for more information.\n"
     ]
    }
   ],
   "source": [
    "#Delete folder if it exists\n",
    "! rm -rm prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a41fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size=8 #8 for prod\n",
    "max_steps=-1 # -1 for prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "506bc740-d613-4429-a96e-0f41bea4bfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.02k/1.02k [00:00<00:00, 3.10MB/s]\n",
      "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/967k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 967k/967k [00:00<00:00, 4.61MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00,  4.67it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1222.83it/s]\n",
      "Generating train split: 100%|██████████| 1000/1000 [00:00<00:00, 99130.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "#For PyTorch memory management add the following code\n",
    "\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\"\n",
    "\n",
    "\n",
    "\n",
    "# Define model, dataset, and new model name\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "guanaco_dataset = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(guanaco_dataset, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042460c2-139e-49c0-af71-68402c2590ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# 4-bit Quantization Configuration\n",
    "#for dev\n",
    "#compute_dtype = getattr(torch, \"float32\")\n",
    "#for Prod\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                  bnb_4bit_quant_type=\"nf4\",\n",
    "                                  bnb_4bit_compute_dtype=compute_dtype,\n",
    "                                  bnb_4bit_use_double_quant=False)\n",
    "\n",
    "# Load model with 4-bit precision\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                             cache_dir=f\"/artifacts/mlflow/{prefix}/llama2-model-cache/\",\n",
    "                                             quantization_config=quant_config,\n",
    "                                             device_map=\"auto\")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12bceec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/peft/utils/other.py:135: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/trl/trainer/sft_trainer.py:166: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 3899.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, \n",
    "                                          cache_dir=f\"/artifacts/mlflow/{prefix}/llama2-model-cache/\",\n",
    "                                          trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Set PEFT Parameters\n",
    "peft_params = LoraConfig(lora_alpha=16,\n",
    "                         lora_dropout=0.1,\n",
    "                         r=64, bias=\"none\",\n",
    "                         task_type=\"CAUSAL_LM\")\n",
    "\n",
    "\n",
    "# Define training parameters\n",
    "training_params = TrainingArguments(output_dir=f\"/artifacts/mlflow/{prefix}/results\",\n",
    "                                    num_train_epochs=1,\n",
    "                                    per_device_train_batch_size=per_device_train_batch_size,\n",
    "                                    gradient_accumulation_steps=1,\n",
    "                                    optim=\"paged_adamw_32bit\",\n",
    "                                    #optim=\"lion_8bit\",\n",
    "                                    save_steps=25,\n",
    "                                    logging_steps=25,\n",
    "                                    learning_rate=2e-4,\n",
    "                                    weight_decay=0.001,\n",
    "                                    fp16=False,\n",
    "                                    bf16=False,\n",
    "                                    max_grad_norm=0.3,\n",
    "                                    max_steps=max_steps,\n",
    "                                    warmup_ratio=0.03,\n",
    "                                    group_by_length=True,\n",
    "                                    lr_scheduler_type=\"constant\",\n",
    "                                    report_to=None)\n",
    "\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = SFTTrainer(model=model,\n",
    "                     train_dataset=dataset,\n",
    "                     peft_config=peft_params,\n",
    "                     dataset_text_field=\"text\",\n",
    "                     max_seq_length=None,\n",
    "                     tokenizer=tokenizer,\n",
    "                     args=training_params,\n",
    "                     packing=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08b24b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning model:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 13:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.465300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.428800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.306900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.342200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.394200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is Leonardo Da Vinci? [/INST] Leonardo da Vinci (1452-1519) was an Italian polymath, artist, inventor, and scientist. He is widely considered one of the greatest painters of all time, and his inventions and designs were centuries ahead of his time. He is known for his famous works such as the Mona Lisa, The Last Supper, and Vitruvian Man. He also made significant contributions to engineering, anatomy, and mathematics. Da Vinci was a true Renaissance man, and his legacy continues to inspire and influence people around the world.\n"
     ]
    }
   ],
   "source": [
    "#https://discuss.pytorch.org/t/how-does-reserved-in-total-by-pytorch-work/70172/33\n",
    "# Train the model\n",
    "#os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING']='1'\n",
    "#os.environ['PYTORCH_CUDA_ALLOC_CONF']='expandable_segments:True'\n",
    "#Force clean the pytorch cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "experiment_name = f'llama2-7b-4bit-lora-sft-{project_id}'\n",
    "exp = mlflow.set_experiment(experiment_name)\n",
    "print(\"Fine-tuning model:\")\n",
    "with mlflow.start_run() as run:\n",
    "    trainer.train()\n",
    "    # Save the model and tokenizer\n",
    "    trainer.model.save_pretrained(f\"/artifacts/mlflow/{prefix}/{new_model}\")\n",
    "    trainer.tokenizer.save_pretrained(f\"/artifacts/mlflow/{prefix}/{new_model}\")\n",
    "\n",
    "# Test the model\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "prompt = \"Who is Leonardo Da Vinci?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d17174e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/artifacts/mlflow/673b4f99011f5130a7ff0019/version0/final_merged_checkpoint/tokenizer_config.json',\n",
       " '/artifacts/mlflow/673b4f99011f5130a7ff0019/version0/final_merged_checkpoint/special_tokens_map.json',\n",
       " '/artifacts/mlflow/673b4f99011f5130a7ff0019/version0/final_merged_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Force garbage collection; kill the kernel and run the first cell and then this cell\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    cache_dir=f\"/artifacts/mlflow/{prefix}/llama2-model-cache/\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, f\"/artifacts/mlflow/{prefix}/{new_model}\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "output_merged_dir = f\"/artifacts/mlflow/{prefix}/final_merged_checkpoint\"\n",
    "os.makedirs(output_merged_dir, exist_ok=True)\n",
    "model.save_pretrained(output_merged_dir)\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.save_pretrained(output_merged_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ff9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shutil,os\n",
    "\n",
    "#deployment_dir = f'/artifacts/mlflow/{model_folder_prefix}/final_merged_checkpoint'\n",
    "#shutil.rmtree(deployment_dir)\n",
    "#shutil.copytree(src=output_merged_dir,dst=deployment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "240ee7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/artifacts/mlflow/673b4f99011f5130a7ff0019/version0/llama-2-7b-chat-guanaco'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"/artifacts/mlflow/{prefix}/{new_model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17e66164-84a3-4b9d-a512-82b471c4b097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d547e767-4938-459b-9279-8a779b7eec1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
