{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce318cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import mlflow\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import gc\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "#print(os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"])\n",
    "project_id = os.environ['DOMINO_PROJECT_ID']\n",
    "new_model = \"llama-2-7b-chat-guanaco\"\n",
    "\n",
    "#Increment the version number each time you retrain it\n",
    "prefix = f\"{project_id}/version0\"\n",
    "model_folder_prefix = f\"{project_id}/model/version0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete folder if it exists\n",
    "! rm -rm prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a41fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size=8 #8 for prod\n",
    "max_steps=-1 # -1 for prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "506bc740-d613-4429-a96e-0f41bea4bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "#For PyTorch memory management add the following code\n",
    "\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\"\n",
    "\n",
    "\n",
    "\n",
    "# Define model, dataset, and new model name\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "guanaco_dataset = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(guanaco_dataset, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "042460c2-139e-49c0-af71-68402c2590ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91ddaba9c6b4c319a101572703444b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837ac4ba8fc343b8aa0b3001de45b847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1002d42d807d45e1ab4a894e56d095a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6403ef69fe4b4eaf12e520aaf3389b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e84ef7c11e417db1e35305ca330e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 4-bit Quantization Configuration\n",
    "#for dev\n",
    "#compute_dtype = getattr(torch, \"float32\")\n",
    "#for Prod\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                  bnb_4bit_quant_type=\"nf4\",\n",
    "                                  bnb_4bit_compute_dtype=compute_dtype,\n",
    "                                  bnb_4bit_use_double_quant=False)\n",
    "\n",
    "# Load model with 4-bit precision\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                             cache_dir=f\"/artifacts/mlflow/{prefix}/llama2-model-cache/\",\n",
    "                                             quantization_config=quant_config,\n",
    "                                             device_map=\"auto\")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12bceec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc57028ff7f490bad38d8a13c38faf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a559d48b1dfb4cb58fa3e3dcc0e1863d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690cbe66c9474a08bbafea918b0e4ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5755680b17544ef815b1b200c066517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7d0eb1cbcd432e964cdf80acced95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/peft/utils/other.py:135: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b7a062ad0d441cbc4682b5b0c0bc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, \n",
    "                                          cache_dir=f\"/artifacts/mlflow/{prefix}/llama2-model-cache/\",\n",
    "                                          trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Set PEFT Parameters\n",
    "peft_params = LoraConfig(lora_alpha=16,\n",
    "                         lora_dropout=0.1,\n",
    "                         r=64, bias=\"none\",\n",
    "                         task_type=\"CAUSAL_LM\")\n",
    "\n",
    "\n",
    "# Define training parameters\n",
    "training_params = TrainingArguments(output_dir=f\"/artifacts/mlflow/{prefix}/results\",\n",
    "                                    num_train_epochs=1,\n",
    "                                    per_device_train_batch_size=per_device_train_batch_size,\n",
    "                                    gradient_accumulation_steps=1,\n",
    "                                    optim=\"paged_adamw_32bit\",\n",
    "                                    #optim=\"lion_8bit\",\n",
    "                                    save_steps=25,\n",
    "                                    logging_steps=25,\n",
    "                                    learning_rate=2e-4,\n",
    "                                    weight_decay=0.001,\n",
    "                                    fp16=False,\n",
    "                                    bf16=False,\n",
    "                                    max_grad_norm=0.3,\n",
    "                                    max_steps=max_steps,\n",
    "                                    warmup_ratio=0.03,\n",
    "                                    group_by_length=True,\n",
    "                                    lr_scheduler_type=\"constant\",\n",
    "                                    report_to=None)\n",
    "\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = SFTTrainer(model=model,\n",
    "                     train_dataset=dataset,\n",
    "                     peft_config=peft_params,\n",
    "                     dataset_text_field=\"text\",\n",
    "                     max_seq_length=None,\n",
    "                     tokenizer=tokenizer,\n",
    "                     args=training_params,\n",
    "                     packing=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08b24b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/25 23:55:09 INFO mlflow.tracking.fluent: Experiment with name 'llama2-7b-4bit-lora-sft-662ad3af892b9833832b4ba0' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning model:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 32:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.466800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.431200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.395700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is Leonardo Da Vinci? [/INST] Leonardo da Vinci (1452-1519) was an Italian polymath, artist, inventor, and scientist. He is widely considered one of the greatest painters of all time, and his inventions and designs were centuries ahead of his time. He is known for his famous works such as the Mona Lisa, The Last Supper, and Vitruvian Man. He also made significant contributions to engineering, anatomy, and mathematics. Da Vinci was a true Renaissance man, and his legacy continues to inspire and influence people around the world.\n"
     ]
    }
   ],
   "source": [
    "#https://discuss.pytorch.org/t/how-does-reserved-in-total-by-pytorch-work/70172/33\n",
    "# Train the model\n",
    "#os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING']='1'\n",
    "#os.environ['PYTORCH_CUDA_ALLOC_CONF']='expandable_segments:True'\n",
    "#Force clean the pytorch cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "experiment_name = f'llama2-7b-4bit-lora-sft-{project_id}'\n",
    "exp = mlflow.set_experiment(experiment_name)\n",
    "print(\"Fine-tuning model:\")\n",
    "with mlflow.start_run() as run:\n",
    "    trainer.train()\n",
    "    # Save the model and tokenizer\n",
    "    trainer.model.save_pretrained(f\"/artifacts/mlflow/{prefix}/{new_model}\")\n",
    "    trainer.tokenizer.save_pretrained(f\"/artifacts/mlflow/{prefix}/{new_model}\")\n",
    "\n",
    "# Test the model\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "prompt = \"Who is Leonardo Da Vinci?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d17174e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650bee1cc98140388eaedb07ce527711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:508: UserWarning: The generation config instance is invalid -- `.validate()` throws warnings and/or exceptions. Fix these issues to save the configuration. This warning will be raised to an exception in v4.34.\n",
      "\n",
      "Thrown during validation:\n",
      "`do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a46596e232748af9664017b98207788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6db2d72989348d78bc70ef5ae621480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349b6b47302441b381a1a97bd39fb19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7975fbfdc184414e8d8d1db34e2506fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77aedd31c3840eb8024cf0420f1c084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/artifacts/mlflow/662ad3af892b9833832b4ba0/version0/final_merged_checkpoint/tokenizer_config.json',\n",
       " '/artifacts/mlflow/662ad3af892b9833832b4ba0/version0/final_merged_checkpoint/special_tokens_map.json',\n",
       " '/artifacts/mlflow/662ad3af892b9833832b4ba0/version0/final_merged_checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Force garbage collection; kill the kernel and run the first cell and then this cell\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    cache_dir=f\"/artifacts/mlflow/{prefix}/llama2-model-cache/\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, f\"/artifacts/mlflow/{prefix}/{new_model}\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "output_merged_dir = f\"/artifacts/mlflow/{prefix}/final_merged_checkpoint\"\n",
    "os.makedirs(output_merged_dir, exist_ok=True)\n",
    "model.save_pretrained(output_merged_dir)\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.save_pretrained(output_merged_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ff9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shutil,os\n",
    "\n",
    "#deployment_dir = f'/artifacts/mlflow/{model_folder_prefix}/final_merged_checkpoint'\n",
    "#shutil.rmtree(deployment_dir)\n",
    "#shutil.copytree(src=output_merged_dir,dst=deployment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "240ee7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/artifacts/mlflow/662ad3af892b9833832b4ba0/version0/llama-2-7b-chat-guanaco'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"/artifacts/mlflow/{prefix}/{new_model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e66164-84a3-4b9d-a512-82b471c4b097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
