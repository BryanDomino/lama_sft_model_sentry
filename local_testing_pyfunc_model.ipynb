{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c35b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import transformers\n",
    "\n",
    "class MyModel(mlflow.pyfunc.PythonModel):\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        import os\n",
    "        import torch\n",
    "        from transformers import (\n",
    "            AutoModelForCausalLM,\n",
    "            AutoTokenizer,\n",
    "            BitsAndBytesConfig\n",
    "        )\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.project_id = os.listdir('/artifacts/mlflow')[0]\n",
    "        compute_dtype = getattr(torch, \"float16\")\n",
    "        quant_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                          bnb_4bit_quant_type=\"nf4\",\n",
    "                                          bnb_4bit_compute_dtype=compute_dtype,\n",
    "                                          bnb_4bit_use_double_quant=False)\n",
    "\n",
    "        ft_model_name = \"final_merged_checkpoint\"\n",
    "        model_cache = \"llama2-model-cache\"\n",
    "        version = \"version0\"\n",
    "        model_tokenizer_path = f\"/artifacts/mlflow/{self.project_id}/{version}/{ft_model_name}\"\n",
    "        cache_dir=f\"/artifacts/mlflow/{self.project_id}/{version}/{model_cache}/\"\n",
    "        print(cache_dir)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_tokenizer_path,\n",
    "                                                                  cache_dir=cache_dir,\n",
    "                                                          quantization_config=quant_config,\n",
    "                                                          device_map=\"auto\"\n",
    "                                                          #device=\"0\"\n",
    "                                                          )\n",
    "        self.model.config.use_cache = False\n",
    "        self.model.config.pretraining_tp = 1\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_tokenizer_path, \n",
    "                                                       cache_dir=f\"/artifacts/mlflow/{self.project_id}/{model_cache}/\",\n",
    "                                                       trust_remote_code=True)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    def predict(self, context, model_input, params=None):\n",
    "        \"\"\"\n",
    "        This method generates prediction for the given input.\n",
    "        \"\"\"\n",
    "        prompt = model_input[\"prompt\"]\n",
    "\n",
    "        if prompt is None:\n",
    "            return 'Please provide a prompt.'\n",
    "        \n",
    "        prompt_template = f\"<s>[INST] {{dialogue}} [/INST]\"\n",
    "            \n",
    "\n",
    "        user_input = f\"<s>[INST] {prompt} [/INST]\"\n",
    "        \n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(self.tokenizer.encode(user_input))\n",
    "        input_length = len(tokens)\n",
    "        \n",
    "        new_tokens = 750\n",
    "        \n",
    "        text = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "        device = \"cuda:0\"\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        generation_config = transformers.GenerationConfig(\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    max_new_tokens = 200\n",
    "                )\n",
    "\n",
    "        outputs = self.model.generate(**inputs, generation_config=generation_config)\n",
    "        llm_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        result = llm_output.replace(f\"[INST] {prompt} [/INST]\", '')\n",
    "        return {'text_from_llm': result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18cbf884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, ColSpec, ParamSchema, ParamSpec\n",
    "\n",
    "# Define input and output schema\n",
    "input_schema = Schema(\n",
    "    [\n",
    "        ColSpec(DataType.string, \"prompt\"),\n",
    "    ]\n",
    ")\n",
    "output_schema = Schema([ColSpec(DataType.string, \"text_from_llm\")])\n",
    "\n",
    "parameters = ParamSchema(\n",
    "    [       \n",
    "    ]\n",
    ")\n",
    "\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=parameters)\n",
    "\n",
    "\n",
    "# Define input example\n",
    "input_example = pd.DataFrame({\"prompt\": [\"What is machine learning?\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faa8e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/mlflow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d499934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Release CUDA memory\n",
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3e79b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import transformers\n",
    "import peft\n",
    "import trl\n",
    "import torch\n",
    "import transformers\n",
    "# Save the model\n",
    "mlflow.pyfunc.save_model(path=\"/tmp/mlflow/\", python_model=MyModel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07cd6f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/artifacts/mlflow/662ad3af892b9833832b4ba0/version0/llama2-model-cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122c50466be84b758d7bbb5de1d6fd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "model = mlflow.pyfunc.load_model(\"/tmp/mlflow/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08726ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict({\"prompt\" : \"Where is Cancun?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c36c4189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_from_llm': ' Cancun is a city in the Mexican state of Quintana Roo, located on the Caribbean coast. It is known for its beautiful beaches, lively nightlife, and rich history. Cancun is a popular tourist destination, attracting visitors from around the world. It is also home to several ancient Mayan ruins, including the famous Chichen Itza. Cancun is located about 40 miles (64 kilometers) south of the city of Playa del Carmen, and is easily accessible by car or airplane.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de17286d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
