{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c35b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import transformers\n",
    "\n",
    "class MyModel(mlflow.pyfunc.PythonModel):\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        import os\n",
    "        import torch\n",
    "        from transformers import (\n",
    "            AutoModelForCausalLM,\n",
    "            AutoTokenizer,\n",
    "            BitsAndBytesConfig\n",
    "        )\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.project_id = os.listdir('/artifacts/mlflow')[0]\n",
    "        compute_dtype = getattr(torch, \"float16\")\n",
    "        quant_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                          bnb_4bit_quant_type=\"nf4\",\n",
    "                                          bnb_4bit_compute_dtype=compute_dtype,\n",
    "                                          bnb_4bit_use_double_quant=False)\n",
    "\n",
    "        ft_model_name = \"final_merged_checkpoint\"\n",
    "        model_cache = \"llama2-model-cache\"\n",
    "        model_tokenizer_path = f\"/artifacts/mlflow/{self.project_id}/{ft_model_name}\"\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_tokenizer_path,\n",
    "                                                                  cache_dir=f\"/artifacts/mlflow/{self.project_id}/{model_cache}/\",\n",
    "                                                          quantization_config=quant_config,\n",
    "                                                          device_map=\"auto\"\n",
    "                                                          #device=\"0\"\n",
    "                                                          )\n",
    "        self.model.config.use_cache = False\n",
    "        self.model.config.pretraining_tp = 1\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_tokenizer_path, \n",
    "                                                       cache_dir=f\"/artifacts/mlflow/{self.project_id}/{model_cache}/\",\n",
    "                                                       trust_remote_code=True)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    def predict(self, context, model_input, params=None):\n",
    "        \"\"\"\n",
    "        This method generates prediction for the given input.\n",
    "        \"\"\"\n",
    "        prompt = model_input[\"prompt\"]\n",
    "\n",
    "        if prompt is None:\n",
    "            return 'Please provide a prompt.'\n",
    "        \n",
    "        prompt_template = f\"<s>[INST] {{dialogue}} [/INST]\"\n",
    "            \n",
    "\n",
    "        user_input = f\"<s>[INST] {prompt} [/INST]\"\n",
    "        \n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(self.tokenizer.encode(user_input))\n",
    "        input_length = len(tokens)\n",
    "        \n",
    "        new_tokens = 750\n",
    "        \n",
    "        text = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "        device = \"cuda:0\"\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        generation_config = transformers.GenerationConfig(\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    max_new_tokens = 200\n",
    "                )\n",
    "\n",
    "        outputs = self.model.generate(**inputs, generation_config=generation_config)\n",
    "        llm_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        result = llm_output.replace(f\"[INST] {prompt} [/INST]\", '')\n",
    "        return {'text_from_llm': result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cbf884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, ColSpec, ParamSchema, ParamSpec\n",
    "\n",
    "# Define input and output schema\n",
    "input_schema = Schema(\n",
    "    [\n",
    "        ColSpec(DataType.string, \"prompt\"),\n",
    "    ]\n",
    ")\n",
    "output_schema = Schema([ColSpec(DataType.string, \"text_from_llm\")])\n",
    "\n",
    "parameters = ParamSchema(\n",
    "    [       \n",
    "    ]\n",
    ")\n",
    "\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=parameters)\n",
    "\n",
    "\n",
    "# Define input example\n",
    "input_example = pd.DataFrame({\"prompt\": [\"What is machine learning?\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/mlflow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d499934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Release CUDA memory\n",
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e79b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import transformers\n",
    "import peft\n",
    "import trl\n",
    "import torch\n",
    "import transformers\n",
    "# Save the model\n",
    "mlflow.pyfunc.save_model(path=\"/tmp/mlflow/\", python_model=MyModel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cd6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "model = mlflow.pyfunc.load_model(\"/tmp/mlflow/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08726ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict({\"prompt\" : \"Where is Cancun?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c4189",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de17286d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
