{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10918109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-miniguanaco\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "project_id = os.environ['DOMINO_PROJECT_ID']\n",
    "#output_dir = \"/mnt/artifacts/llama2-results\"\n",
    "output_dir = f\"/artifacts/mlflow/{project_id}/llama2-results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = 225\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=f\"/artifacts/mlflow/{project_id}/llama2-model-cache/\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=[\"mlflow\"],\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c669e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'llama2-7b-4bit-lora-ft-v2'\n",
    "exp = mlflow.set_experiment(experiment_name)\n",
    "print(\"Fine-tuning model:\")\n",
    "with mlflow.start_run() as run:\n",
    "    # Start training - Skipping this step\n",
    "    #trainer.train()\n",
    "    # Save trained model\n",
    "    print(f\"Saving Model to {output_dir}/{new_model}\")\n",
    "    trainer.model.save_pretrained(f\"{output_dir}/{new_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{output_dir}/{new_model}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1273ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eccee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF']='expandable_segments:True'\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    cache_dir=f\"/artifacts/mlflow/{project_id}/llama2-model-cache/\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, f\"{output_dir}/{new_model}\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "output_merged_dir = f\"/artifacts/mlflow/{project_id}/final_merged_checkpoint\"\n",
    "os.makedirs(output_merged_dir, exist_ok=True)\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.save_pretrained(output_merged_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27062e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "from transformers import GenerationConfig \n",
    "\n",
    "prompt = \"What should i do in Paris?\"\n",
    "\n",
    "text = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            max_new_tokens = 200\n",
    "        )\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "llm_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "cleaned_output = llm_output.replace(f\"[INST] {prompt} [/INST]\", '')\n",
    "print(cleaned_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d40047",
   "metadata": {},
   "source": [
    "## Model Class\n",
    "\n",
    "The `MyModel` is a Pyfunc model that will be registered to MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9733ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "        os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir\n",
    "        self.project_id = os.listdir('/artifacts/mlflow')[0]\n",
    "        # Load the ctranslate model\n",
    "        model_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # model_path = '/mnt/artifacts/llama2/final_merged_checkpoint/'        \n",
    "        model_path = f'/artifacts/mlflow/{self.project_id}/artifacts/llama2-ct'\n",
    "        model_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # load the ctranslate model\n",
    "        self.generator = ctranslate2.Generator(model_path, device=model_device)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(f'/artifacts/mlflow/{self.project_id}/llama-2-7b-miniguanaco')\n",
    "\n",
    "\n",
    "    prompt_template = f\"<s>[INST] {{dialogue}} [/INST]\"\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        \"\"\"\n",
    "        This method generates prediction for the given input.\n",
    "        \"\"\"\n",
    "        prompt = model_input[\"prompt\"][0]\n",
    "\n",
    "        if prompt is None:\n",
    "            return 'Please provide a prompt.'\n",
    "            \n",
    "        # Construct the prompt for the model\n",
    "        user_input = prompt_template.format(dialogue=prompt)\n",
    "    \n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(user_input))\n",
    "        input_length = len(tokens)\n",
    "        # new_tokens = round(pct_new_tokens*input_length)\n",
    "        new_tokens = 750\n",
    "        tokens_per_sec = 0\n",
    "        start_time = time.time()\n",
    "        results = generator.generate_batch([tokens], sampling_topk=10, max_length=new_tokens, include_prompt_in_result=False)\n",
    "        end_time = time.time()\n",
    "        output_text = tokenizer.decode(results[0].sequences_ids[0])\n",
    "        tokens_per_sec = round(new_tokens / (end_time - start_time),3)\n",
    "    \n",
    "        # return {'text_from_llm': output_text, 'tokens_per_sec': tokens_per_sec}\n",
    "        return {'text_from_llm': output_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c802fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, ColSpec, ParamSchema, ParamSpec\n",
    "\n",
    "# Define input and output schema\n",
    "input_schema = Schema(\n",
    "    [\n",
    "        ColSpec(DataType.string, \"prompt\"),\n",
    "    ]\n",
    ")\n",
    "output_schema = Schema([ColSpec(DataType.string, \"text_from_llm\")])\n",
    "\n",
    "parameters = ParamSchema(\n",
    "    [       \n",
    "    ]\n",
    ")\n",
    "\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=parameters)\n",
    "\n",
    "\n",
    "# Define input example\n",
    "input_example = pd.DataFrame({\"prompt\": [\"What is machine learning?\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253fa401",
   "metadata": {},
   "source": [
    "## Create a registered model if it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170b46ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "client = mlflow.MlflowClient()\n",
    "model_name=\"lama2-sft\"\n",
    "registered_model = None\n",
    "try:\n",
    "    registered_model = client.create_registered_model(model_name)\n",
    "except:\n",
    "     registered_model = client.get_registered_model(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42784068",
   "metadata": {},
   "source": [
    "## Create a Model Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fab69f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "from mlflow.store.artifact.runs_artifact_repo import RunsArtifactRepository\n",
    "\n",
    "# Get the current base version of torch that is installed, without specific version modifiers\n",
    "torch_version = torch.__version__.split(\"+\")[0]\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import peft\n",
    "import trl\n",
    "\n",
    "# Start an MLflow run context and log the MPT-7B model wrapper along with the param-included signature to\n",
    "# allow for overriding parameters at inference time\n",
    "with mlflow.start_run() as run:\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        model_name,\n",
    "        python_model=MyModel(),\n",
    "        # NOTE: the artifacts dictionary mapping is critical! This dict is used by the load_context() method in our MyModel() class.\n",
    "        artifacts={\"snapshot\": '/mnt/code/'},\n",
    "        pip_requirements=[\n",
    "            f\"torch=={torch_version}\",\n",
    "            f\"transformers=={transformers.__version__}\",                        \n",
    "            f\"peft=={peft.__version__}\",\n",
    "            f\"trl=={trl.__version__}\",\n",
    "            \"einops\",\n",
    "            \"sentencepiece\",\n",
    "        ],\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )\n",
    "    runs_uri = model_info.model_uri\n",
    "    print(runs_uri)\n",
    "    # Create a new model version of the RandomForestRegression model from this run\n",
    "    \n",
    "    model_src = RunsArtifactRepository.get_underlying_uri(runs_uri)\n",
    "    mv = client.create_model_version(model_name, model_src, run.info.run_id)\n",
    "    print(\"Name: {}\".format(mv.name))\n",
    "    print(\"Version: {}\".format(mv.version))\n",
    "    print(\"Description: {}\".format(mv.description))\n",
    "    print(\"Status: {}\".format(mv.status))\n",
    "    print(\"Stage: {}\".format(mv.current_stage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70144371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
